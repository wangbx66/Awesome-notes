## 2013 Mikolov W2V Notes  
### Hierarchical Softmax
This is basically the standard whenever the network needs to predict a term when the dictionary is big, say, 250,000 words. If we calculate the activation value of all the nodes and then the softmax probability, the computation cost would be too high. Hence, we encode the tree using a Huffman tree [^footnote]. Afterwards we initiate L many neurons where L is the length of the tree, where the output of each neuron represents the decision of left/right from the root down to the leaf. And the word corresponding to the leaf of the roll-out, is the prediction of the model.

[^footnote]: Recall how to make a Huffman tree: Firstly calculate the frequency of each word. Repeatly at each turn, assign a respective '0' and '1' as the prefix of both the least frequent terms, and combine the two to be one new term. Until there's only one term which appears to be the root of the Huffman tree, the assignment achieves the shortest coding length possible.

### Negative Sampling
Plain training process tries to maximize the probability of predicting the correct term. To make the model more power at distinguishing the correctness and randomness, the model instead maximizes the difference of log probabilities of the ground truth and some random words. To see the details of drawing random words, checkout the paper section.

### Subsample Frequent Words
The colocation of frequent words with other words are meaningless. Hence the model discard each word with probability (1 - f**(-1/2)).

## Paid
### Extract Phrases
Just require TMI to be within the top n many bigrams. TMI is Pr(a,b)/Pr(a)Pr(b). Also require characters of each word, non-stop word and non adv/determiner POS.

### Topic-phrase align
The goal is to find a good semantic phrase. The phrase should cover the entire topic, and discriminative among topics. Assume Pr(w|l) is the topic model generated by phrase l, the dis-similarity between l and \beta is the KL-Divergence between l and \beta. To calculate the KL, we notice that Pr(w|\beta)/Pr(w|l) = Pr(w|C)/Pr(w|l,C)\*Pr(w|\beta)/Pr(w|C)\*Pr(w|l,C)/Pr(w|l). It then can be approximated by -PMI(w,l). In fact, the second term is irrelated to \beta (and we're trying to rank \beta). Also the third term can be regarded as 0, since l itself is generated from C, hence has no additional effect. The first term equals Pr(w|C)Pr(l|C)/Pr(w,l|C), since Pr(w,l|C) = Pr(l|C)Pr(w|l,C)

### Prioritizing Issue

We consider not only the similarity. also to minimize the similarity with other topics. The objective function would be sim(phrase, \beta) - \alpha sum_(\beta^\prime)sim(phrase, \beta^\prime)

## 2015 T2V Notes
### LDA
It firstly conduct LDA to get the topics. LDA is a generative model which assumes the following generative process
1. It first draw a \theta from dir(\alpha) for each document. \theta is the categorical variable which decides the probability distribution the topic a word would belongs to
2. It then draw a \fi from dir(\beta) for each topics. \fi is the categorical variable which decides the probability distribution over words which represents the topic
3. For the generation of each document, repeat the following: generate a topic according to \theta, then a word according to the \fi that corresponds to the previously generated topic.

### T2V Network
Take the skip-gram model in W2V. Each time it tries to predict the surrounding words using the current word, it also takes into consider the topic associated with the current word as a second input. Both inputs are embedded and concatenated with each other before fed into the hidden layer. The embeddings of both topics and words are learned in this way.

### Code Details
It first runs CountVectorizer.fit_transmit() to transmit a list of documents/strings into a sparse matrix and then fed into LDA. Afterwise it extracts the components of LDA which is a n_topic by n_terms matrix for term distribution within topics. Then it calculates the greatest value of each term over all topics and associate that topic to the term.

After that it tags the same set of documents, such that each term is tagged as the associated topic. Then call gensim.models.doc2vec and train that list of tagged documents would be fine. Here we hacked the code of doc2vec, where the tags can be arbitrary. For its originally the tags are just the document IDs.

## 2016 Topicvec Notes
### It's basically an LDA. Within the model the words are embedded.

## Doc2Vec Notes
The most naive way: just learn W2V and average all words in the paragraph to yield the paragraph vector.  
Instead we make it a little bit subtler. Recall that in c-bow each word in the paragraph is predicted using the context i.e. the k words surrounding it. We treat the paragraph itself to be an additional word which is the context of all the words of that paragraph. Here, the document is represented by its ID and is mapped through an embedding layer.

## Tweet2Vec
### Make It Character Based
Tweets contains so many words that's not in a regular "dictionary". Those customized words would greatly affect the performance of a word based model (basically all models) e.g. "great" and "greeeeeat" is very similar semantically, but treated differently and, the later would not be well modeled since it's frequency is relatively low. In Tweet2Vec it would process the whole Tweet character-by-character, resulting a more robust model which handles the noisy induced by customized words.

### Bi-GRU
It predicts the hastags of the tweet. It firstly apply a Bi-GRU to the tweet, character by character, and get the output vectors of both the hidden layers. Note that the output of the hidden layer is the output value of the GRU after processing the last character of the tweet. After that it concatenate both the vector into one and apply logistic regressions to get the probility that a tag is associated with this tweet. Note it output L many probabilities, if there's L many possible hashtags for a tweet.

### Experiment Details
1. Process only one language, in the paper, English.
2. Remove those very frequent hasgtags which is likely to be auto-generated. Also those infrequent.
3. The learning rate is halved everytime the validation set precision increases by less than 0.01 % from one epoch to the next

